{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b59d16f",
   "metadata": {},
   "source": [
    "# CME 250 Machine Learning Project 2\n",
    "## -- Sweekar Bengaluru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3195333",
   "metadata": {},
   "source": [
    "Data Source: The data for this project is obtained from https://www.kaggle.com/competitions/cme250-final-project/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6eb52",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The project is on using Machine learning algorithms to analyse the patient data and predict whether the patient would progress to severe dengue case or not. Algorithm is built to correlate over 3500 gene signatures from 300 observations to predict if the patient progresses to severe case of dengue. \n",
    "\n",
    "The project report is sumitted on kaggle under the username: s.sweekar-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a6fc0",
   "metadata": {},
   "source": [
    "### Prediction algorithm development:\n",
    "Since this is a Supervised learning and a classification problem, Logistic regression, Random forest and neural net algorithms were tried out. Initially, a simple logistic regression algorithm was built and then dimension reduction techniques, cross validation techniques were applied. In addition, different solver and regularisation/penalisation methods were explored. lbfgs, newton-cg, sag, saga, liblinear solver models were tried, L1 and L2 regularisation technique was also applied. For the given dataset, with iteration count of 1000, lbfgs, C=2.5(Regularisation metric, smaller the value, higher the extent of regularisation) turned out to provide best results during validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcb3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d80a516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data and checking data quality\n",
    "data = pd.read_csv(\"train1.csv\",index_col = \"SampleID\")\n",
    "data.head()\n",
    "X = data.drop(['class','dataset'],axis='columns')\n",
    "y = data['class']\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().sum()/len(X)>0:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619f00b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7554593616746349\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.77\n",
      "[[91 18]\n",
      " [23 48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82       109\n",
      "           1       0.73      0.68      0.70        71\n",
      "\n",
      "    accuracy                           0.77       180\n",
      "   macro avg       0.76      0.76      0.76       180\n",
      "weighted avg       0.77      0.77      0.77       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data split and logistic regression application\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=0)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "clf = LogisticRegression(penalty='l2',solver='lbfgs',C=2.5,max_iter=2500,random_state=0).fit(X_train, y_train)\n",
    "y_val_logit_hat = clf.predict(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_val_logit_hat, pos_label=1)\n",
    "auc_logit = metrics.auc(fpr, tpr)\n",
    "print(auc_logit)\n",
    "print(\"Train accuracy:\", np.round(accuracy_score(y_train, \n",
    "                                                 clf.predict(X_train)), 2))\n",
    "print(\"Test accuracy:\", np.round(accuracy_score(y_test, y_val_logit_hat), 2))\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,clf.predict(X_test)))\n",
    "print(classification_report(y_test,clf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f158b8",
   "metadata": {},
   "source": [
    "The recall and precision results are reasonable. These are centered around 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c819cb",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b686cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test1.csv\",index_col = \"SampleID\")\n",
    "data_test.head()\n",
    "X_test_final = data_test.drop(['dataset'],axis='columns')\n",
    "y_predict = pd.DataFrame(clf.predict_proba(X_test_final),columns=clf.classes_)[1]\n",
    "y_pred_df = pd.DataFrame(y_predict)\n",
    "y_pred_df.columns = [\"class\"]\n",
    "y_pred_df.index = data_test.index\n",
    "y_pred_df.to_csv(\"PCA_Logistic_Benchmark_8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462db8c7",
   "metadata": {},
   "source": [
    "With this code, the AUC on the unseen data turned out to be 0.77. In next iteration, Dimension reduction techniques were applied and it was found that 17 dimensions were required to accurately represent data. With this reduced dimension and cross validation, AUC on unseen data turned out to be 0.45 which was surprising!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b840e",
   "metadata": {},
   "source": [
    "In next iteration, PCA and cross validation was used, but the logistic regression model was trained on the entire data and the AUC for unseen data turned out to be 0.8.\n",
    "Similarly, random forest and tree pruning techniques were applied and results for AUC for unseen data was around 0.6. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185a4783",
   "metadata": {},
   "source": [
    "Then a neural net model was trained on the entire data using MLP classifier available in Scikit learn. Different neural net architectures were tried and for varying depth and breadth. The best results were centered arouund 0.6 AUC for unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a90f79",
   "metadata": {},
   "source": [
    "Finally, referencing paper, 8 significant gene signatures were identified and this was used to train logistic regression and MLP classifier (Neural net model with 7,7,7). This returned the results of 0.77 and 0.76 respectively on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05096d8",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "Possile improvement could be on iterating on the neural net architecture so as to improve prediction, reduce overfitting. Data cleanup would also help by removing erranoeus and outlier datapoints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
